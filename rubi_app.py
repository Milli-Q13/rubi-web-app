import streamlit as st
import zipfile
import xml.etree.ElementTree as ET
import jaconv
import json
import os
from pathlib import Path
from sudachipy import tokenizer, dictionary

# SudachiåˆæœŸåŒ–
tokenizer_obj = dictionary.Dictionary(dict_type="core").create()
mode = tokenizer.Tokenizer.SplitMode.C

# overrideè¾æ›¸ã®èª­ã¿è¾¼ã¿
def load_override_dict():
    if os.path.exists("override.json"):
        with open("override.json", "r", encoding="utf-8") as f:
            return json.load(f)
    return {}

# overrideè¾æ›¸ã®ä¿å­˜
def save_override_dict(override_dict):
    with open("override.json", "w", encoding="utf-8") as f:
        json.dump(override_dict, f, ensure_ascii=False, indent=2)

# èªå¥æŠ½å‡ºé–¢æ•°
def extract_terms(file_path, override_dict):
    with zipfile.ZipFile(file_path, "r") as docx:
        with docx.open("word/document.xml") as file:
            tree = ET.parse(file)
            root = tree.getroot()
            ns = {"w": "http://schemas.openxmlformats.org/wordprocessingml/2006/main"}
            texts = [node.text for node in root.findall(".//w:t", ns) if node.text]

    full_text = "".join(texts)
    words = {}

    for m in tokenizer_obj.tokenize(full_text, mode):
        surface = m.surface()
        if len(surface) <= 1 or surface in words:
            continue
        if all('\u3040' <= ch <= '\u309F' for ch in surface):
            continue
        if surface in override_dict:
            reading = override_dict[surface]
        else:
            reading = jaconv.kata2hira(m.reading_form())
        if surface == reading:
            continue
        words[surface] = reading

    return [{"word": w, "reading": r} for w, r in words.items()]

# TSVä¿å­˜
def save_tsv(terms, file_name):
    base_name = Path(file_name).stem
    save_dir = Path("ãƒ«ãƒ“ãƒ‡ãƒ¼ã‚¿")
    save_dir.mkdir(exist_ok=True)
    tsv_path = save_dir / f"{base_name}ï¼ˆãƒ«ãƒ“ï¼‰.tsv"

    with open(tsv_path, "w", encoding="cp932") as f:
        for term in terms:
            f.write(f"{term['word']}\t{term['reading']}\n")
    return tsv_path

# Streamlit UI
st.title("ğŸ“˜ ãƒ«ãƒ“ç·¨é›†ãƒ„ãƒ¼ãƒ«ï¼ˆStreamlitç‰ˆï¼‰")

override_dict = load_override_dict()

uploaded_files = st.file_uploader("Wordãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆè¤‡æ•°å¯ï¼‰", type=["docx"], accept_multiple_files=True)

if uploaded_files:
    for i, uploaded_file in enumerate(uploaded_files):
        st.markdown(f"### ğŸ“„ {uploaded_file.name}")

        file_bytes = uploaded_file.read()
        temp_path = Path(f"temp_{i}.docx")  # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ãƒ¦ãƒ‹ãƒ¼ã‚¯ã«

        with open(temp_path, "wb") as f:
            f.write(file_bytes)

        terms = extract_terms(temp_path, override_dict)
        st.success(f"{len(terms)} ä»¶ã®èªå¥ã‚’æŠ½å‡ºã—ã¾ã—ãŸ")

        edited_terms = st.data_editor(
            terms,
            column_config={
                "word": "èªå¥",
                "reading": "èª­ã¿"
            },
            num_rows="dynamic",
            key=f"editor_{i}"  # è¤‡æ•°ã‚¨ãƒ‡ã‚£ã‚¿ã«ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚­ãƒ¼ã‚’
        )


    if st.button("ğŸ“„ TSVä¿å­˜"):
        tsv_path = save_tsv(edited_terms, uploaded_file.name)
        st.info(f"ä¿å­˜å®Œäº†ï¼š{tsv_path}")

    st.markdown("---")
    st.subheader("ğŸ“š è¾æ›¸ç·¨é›†ï¼ˆoverride.jsonï¼‰")

    dict_items = [{"word": k, "reading": v} for k, v in override_dict.items()]
    edited_dict = st.data_editor(dict_items, num_rows="dynamic")

    if st.button("ğŸ’¾ è¾æ›¸ä¿å­˜"):
        new_dict = {item["word"]: item["reading"] for item in edited_dict if item["word"] and item["reading"]}
        save_override_dict(new_dict)
        st.success("è¾æ›¸ã‚’ä¿å­˜ã—ã¾ã—ãŸ")


